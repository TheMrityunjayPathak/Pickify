{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Adding `utils` directory to `PYTHONPATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../utils\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Reading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing load_csv function from read_data module\n",
    "from read_data import load_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cleaned data\n",
    "cleaned_df = load_csv('clean_data', 'cleaned_data.csv')\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "##### Vectorization\n",
    "- It is the process of converting text into numbers, specifically into vectors so that a computer can understand and work with them.\n",
    "- Computers don't understand text directly, they understand numbers.\n",
    "- So, before we feed text to a machine learning model, we need to turn our words into numerical form.\n",
    "- Imagine trying to train a model to detect whether a message is positive or negative.\n",
    "- If you just give it the sentence :\n",
    "> \"I love this movie!\"\n",
    "- The model doesn't know what \"love\" or \"movie\" means.\n",
    "- You need to translate those words into numbers.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Let's suppose we have three sentences which we are going to convert into numbers.\n",
    "- `I love NLP`\n",
    "- `I love deep learning`\n",
    "- `NLP is fun`\n",
    "\n",
    "##### Step 1 : Build a Vocabulary (Unique Words)\n",
    "- From all 3 sentences, list every unique word :\n",
    "| Word         |\n",
    "|--------------|\n",
    "| I            |\n",
    "| love         |\n",
    "| NLP          |\n",
    "| deep         |\n",
    "| learning     |\n",
    "| is           |\n",
    "| fun          |\n",
    "\n",
    "- So the vocabulary size is 7.\n",
    "\n",
    "##### Step 2 : Bag of Words (BoW)\n",
    "Now we represent each sentence as a vector of word counts using the vocabulary above.\n",
    "| Sentence                 | I | love | NLP | deep | learning | is | fun |\n",
    "|--------------------------|---|------|-----|------|----------|----|-----|\n",
    "| I love NLP | 1 | 1 | 1   | 0    | 0        | 0  | 0   |\n",
    "| I love deep learning | 1 | 1    | 0   | 1    | 1        | 0  | 0   |\n",
    "| NLP is fun | 0 | 0    | 1   | 0    | 0        | 1  | 1   |\n",
    "\n",
    "- Each row = one sentence.\n",
    "- Each column = one word from the vocabulary.\n",
    "- Each cell = how many times that word appeared in that sentence.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### Stop Words\n",
    "- Stop words are the common, frequently occurring words in a language that don't carry much meaning on their own.\n",
    "- **Example** : `the`, `is`, `in`, `on`, `and`, `a`, `an`, `to`, `of`, `with`, `for`, `from`, `that`, `this`, `it`, etc.\n",
    "- They are often removed from text because:\n",
    "    - They don't help distinguish between documents/sentences.\n",
    "    - They add noise and increase vector size in models like Bag of Words (BoW).\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### What is Stemming?\n",
    "- Stemming is the process of reducing a word to its base or root form (called the \"stem\").\n",
    "| Word | Stem |\n",
    "|:--:|:---:|\n",
    "| running | run |\n",
    "| runs | run |\n",
    "| runner | runner |\n",
    "| studied | studi |\n",
    "| studies |\tstudi |\n",
    "\n",
    "##### Problem with BoW without Stemming\n",
    "BoW treats every unique word as different, even if they are grammatically related. Like :\n",
    "1. `\"He is running in the park\"`\n",
    "2. `\"She runs every day\"`\n",
    "- Without stemming, `running` ≠ `runs`.\n",
    "- So BoW thinks these are completely different words, even though they refer to the same root concept: \"`run`\".\n",
    "\n",
    "##### BoW Matrix (Without Stemming)\n",
    "Vocabulary: `[\"he\", \"is\", \"running\", \"in\", \"the\", \"park\", \"she\", \"runs\", \"every\", \"day\"]`\n",
    "| Sentence          | he | is | running | in | the | park | she | runs | every | day |\n",
    "|-------------------|----|----|---------|----|-----|------|-----|------|-------|-----|\n",
    "| Sentence 1        | 1  | 1  | 1       | 1  | 1   | 1    | 0   | 0    | 0     | 0   |\n",
    "| Sentence 2        | 0  | 0  | 0       | 0  | 0   | 0    | 1   | 1    | 1     | 1   |\n",
    "\n",
    "These vectors are very different even though the meaning is similar.\n",
    "\n",
    "##### Benefit of Stemming\n",
    "Stemming reduces related words to their common root form, like :\n",
    "1. `running`, `runs`, `ran` → `run`\n",
    "2. `studies`, `studying`, `studied` → `studi`\n",
    "- Now BoW will group them together!\n",
    "\n",
    "##### BoW Matrix (With Stemming)\n",
    "New Vocabulary: `[\"he\", \"is\", \"run\", \"in\", \"the\", \"park\", \"she\", \"every\", \"day\"]`\n",
    "| Sentence          | he | is | run | in | the | park | she | every | day |\n",
    "|-------------------|----|----|-----|----|-----|------|-----|-------|-----|\n",
    "| Sentence 1        | 1  | 1  | 1   | 1  | 1   | 1    | 0   | 0     | 0   |\n",
    "| Sentence 2        | 0  | 0  | 1   | 0  | 0   | 0    | 1   | 1     | 1   |\n",
    "\n",
    "Now \"`running`\" and \"`runs`\" are treated the same as \"`run`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stem function from text_stemming module\n",
    "from text_stemming import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stemming on tags column\n",
    "cleaned_df['tags'] = cleaned_df['tags'].apply(stem)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Applying Bag of Words on `tags` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying BOW on tags column\n",
    "vectors = cv.fit_transform(cleaned_df['tags']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of vectors array\n",
    "vectors.shape\n",
    "# Here 4381 rows are the tags for each movie\n",
    "# And, 5000 columns are the most common 5000 words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique words (Top 5000 most common words)\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all stop words from tags column\n",
    "print(cv.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Calculating Cosine Similarity of Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### Cosine Similarity\n",
    "- In the context of Bag of Words (BoW), cosine similarity is a metric used to measure the similarity between two text vectors.\n",
    "- Since BoW transforms text into vectors, cosine similarity allows you to compare how similar two text are, based on their word distributions.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### Why Cosine Similarity?\n",
    "- Cosine similarity is often preferred because it focuses on the angle between vectors rather than their magnitude.\n",
    "- This means it's more concerned with how the words are distributed in each document rather than how long or short the documents are.\n",
    "- It ranges between -1 and 1:\n",
    "    - **1** means the vectors are identical (very similar).\n",
    "    - **0** means the vectors are orthogonal (completely dissimilar).\n",
    "    - **-1** would indicate completely opposite vectors, though this is rare in typical document similarity scenarios.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### Formula for Cosine Similarity\n",
    "Given two vectors, $A$ and $B$, the cosine similarity is calculated as :\n",
    "$$\\text{cosine similarity} = \\frac{A \\cdot B}{|A| \\cdot |B|}$$\n",
    "where:\n",
    "- $A \\cdot B$ is the dot product of the two vectors.\n",
    "- $|A|$ and $|B|$ are the euclidean norms (magnitude) of the vectors.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### Example\n",
    "Let's say we have two documents and after applying Bag of Words, we get the following vectors (with word counts) :\n",
    "| Word  | Doc 1 | Doc 2 |\n",
    "|-------|-------|-------|\n",
    "| cat   | 2     | 1     |\n",
    "| dog   | 1     | 2     |\n",
    "\n",
    "Now, let's compute the cosine similarity between `Doc 1` and `Doc 2` :\n",
    "\n",
    "**Dot product** of `Doc 1` and `Doc 2`:\n",
    "$$(2 \\times 1) + (1 \\times 2) + (0 \\times 1) = 2 + 2 + 0 = 4$$\n",
    "\n",
    "**Magnitude of `Doc 1`**:\n",
    "$$\\text{Doc 1} = \\sqrt{(2^2) + (1^2) + (0^2)} = \\sqrt{4 + 1} = \\sqrt{5}$$\n",
    "\n",
    "**Magnitude of `Doc 2`**:\n",
    "$$\\text{Doc 2} = \\sqrt{(1^2) + (2^2) + (1^2)} = \\sqrt{1 + 4 + 1} = \\sqrt{6}$$\n",
    "\n",
    "**Cosine Similarity**:\n",
    "$$\\text{cosine similarity} = \\frac{4}{\\sqrt{5} \\times \\sqrt{6}} \\approx \\frac{4}{\\sqrt{30}} \\approx 0.7303$$\n",
    "\n",
    "This means the two documents are quite similar based on the words they share.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### Cosine Distance\n",
    "The cosine distance is just the inverse of cosine similarity :\n",
    "$$\\text{cosine distance} = 1 - \\text{cosine similarity}$$\n",
    "So, in the example above :\n",
    "$$\\text{cosine distance} = 1 - 0.7303 = 0.2697$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of similarity array\n",
    "# It contains similarity between one vector and every other vector \n",
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of 1st movie with every other movie\n",
    "similarity[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Exporting `similarity` as `pickle` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing export_as_pickle function from serialization module\n",
    "from serialization import export_as_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting similarity as pickle file \n",
    "export_as_pickle(similarity, 'models', 'similarity.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
